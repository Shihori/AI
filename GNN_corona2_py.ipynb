{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvzf3ViXOHxDYQeF/zKwDx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shihori/AI/blob/main/GNN_corona2_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gbrgHgvAMpLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91fd7210-a895-4633-bcd9-1716aab96d22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/GNN-corona2/\n",
        "%ls -a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWl_RMdJiH_E",
        "outputId": "f02c8080-7cee-4fa3-d500-11eb0d0d3994"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/GNN-corona2\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  GNN-corona2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "\n",
        "def image_to_graph_data(image_path, label):\n",
        "\n",
        "    try:\n",
        "        img = Image.open(image_path).convert('RGB').resize((32, 32))\n",
        "        img_array = np.array(img) / 255.0\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "    height, width, channels = img_array.shape\n",
        "    num_nodes = height * width\n",
        "\n",
        "    x = torch.tensor(img_array.reshape(num_nodes, channels), dtype=torch.float)\n",
        "\n",
        "    edge_index = []\n",
        "    for r in range(height):\n",
        "        for c in range(width):\n",
        "            node_idx = r * width + c\n",
        "\n",
        "            if c + 1 < width:\n",
        "                edge_index.append([node_idx, node_idx + 1])\n",
        "                edge_index.append([node_idx + 1, node_idx])\n",
        "\n",
        "            if r + 1 < height:\n",
        "                edge_index.append([node_idx, (r + 1) * width + c])\n",
        "                edge_index.append([(r + 1) * width + c, node_idx])\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "\n",
        "    y = torch.tensor([label], dtype=torch.long)\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, y=y)\n",
        "    return data\n",
        "\n",
        "class GCNForImageClassification(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "\n",
        "        self.classifier = torch.nn.Linear(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        graph_level_features = global_mean_pool(x, batch)\n",
        "\n",
        "\n",
        "        out = self.classifier(graph_level_features)\n",
        "        return out\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_image_dir = 'data'\n",
        "\n",
        "    category_to_label = {}\n",
        "    label_counter = 0\n",
        "    all_image_paths = []\n",
        "    all_labels = []\n",
        "\n",
        "    if not os.path.exists(base_image_dir):\n",
        "        print(f\"Error: Directory '{base_image_dir}' not found.\")\n",
        "        print(\"Please create subdirectories like 'data/category_A', 'data/category_B' and place images inside.\")\n",
        "        exit()\n",
        "\n",
        "    for category_name in sorted(os.listdir(base_image_dir)):\n",
        "        category_dir = os.path.join(base_image_dir, category_name)\n",
        "        if os.path.isdir(category_dir):\n",
        "            category_to_label[category_name] = label_counter\n",
        "            print(f\"Mapping '{category_name}' to label {label_counter}\")\n",
        "            for img_file in os.listdir(category_dir):\n",
        "                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    all_image_paths.append(os.path.join(category_dir, img_file))\n",
        "                    all_labels.append(label_counter)\n",
        "            label_counter += 1\n",
        "\n",
        "    if not all_image_paths:\n",
        "        print(\"No image files found in the specified categories. Please check your data directory.\")\n",
        "        exit()\n",
        "\n",
        "    if len(all_image_paths) > 100:\n",
        "        indices = np.random.choice(len(all_image_paths), 100, replace=False)\n",
        "        all_image_paths = [all_image_paths[i] for i in indices]\n",
        "        all_labels = [all_labels[i] for i in indices]\n",
        "        print(f\"Using a random subset of 100 images from {len(all_image_paths)} available.\")\n",
        "    else:\n",
        "        print(f\"Using all {len(all_image_paths)} images found.\")\n",
        "\n",
        "    graph_datasets = []\n",
        "    for i, img_path in enumerate(all_image_paths):\n",
        "        graph_data = image_to_graph_data(img_path, all_labels[i])\n",
        "        if graph_data:\n",
        "            graph_datasets.append(graph_data)\n",
        "\n",
        "    if not graph_datasets:\n",
        "        print(\"No valid graph data could be created from images. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    print(f\"Successfully converted {len(graph_datasets)} images to graph data.\")\n",
        "\n",
        "    train_data, test_data = train_test_split(graph_datasets, test_size=0.2, random_state=42)\n",
        "    print(f\"Train data size: {len(train_data)}, Test data size: {len(test_data)}\")\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "    in_channels = graph_datasets[0].x.shape[1]\n",
        "    hidden_channels = 64\n",
        "    num_classes = len(category_to_label)\n",
        "\n",
        "    model = GCNForImageClassification(in_channels, hidden_channels, num_classes)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    def train(model, train_loader, optimizer, criterion, device):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total_samples = 0\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = criterion(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == data.y).sum().item()\n",
        "            total_samples += data.y.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = correct / total_samples\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def test(model, loader, device):\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for data in loader:\n",
        "                data = data.to(device)\n",
        "                out = model(data)\n",
        "                pred = out.argmax(dim=1)\n",
        "                correct += (pred == data.y).sum().item()\n",
        "                total_samples += data.y.size(0)\n",
        "        accuracy = correct / total_samples\n",
        "        return accuracy\n",
        "\n",
        "    epochs = 100\n",
        "    print(f\"\\n--- Starting training on {device} ---\")\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
        "        test_acc = test(model, test_loader, device)\n",
        "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "    print(\"\\n--- Training complete ---\")\n",
        "    print(f\"Final Test Accuracy: {test(model, test_loader, device):.4f}\")\n",
        "\n",
        "    print(\"\\nCategory to Label Mapping:\")\n",
        "    for category, label in category_to_label.items():\n",
        "        print(f\"  {category}: {label}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping 'category_A' to label 0\n",
            "Mapping 'category_B' to label 1\n",
            "Using all 100 images found.\n",
            "Successfully converted 100 images to graph data.\n",
            "Train data size: 80, Test data size: 20\n",
            "\n",
            "--- Starting training on cpu ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Loss: 0.7031, Train Acc: 0.4750, Test Acc: 0.4000\n",
            "Epoch: 002, Train Loss: 0.6934, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 003, Train Loss: 0.6960, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 004, Train Loss: 0.6904, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 005, Train Loss: 0.6907, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 006, Train Loss: 0.6904, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 007, Train Loss: 0.6925, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 008, Train Loss: 0.6914, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 009, Train Loss: 0.6876, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 010, Train Loss: 0.6897, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 011, Train Loss: 0.6914, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 012, Train Loss: 0.6934, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 013, Train Loss: 0.6886, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 014, Train Loss: 0.6899, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 015, Train Loss: 0.6914, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 016, Train Loss: 0.6913, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 017, Train Loss: 0.6945, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 018, Train Loss: 0.6912, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 019, Train Loss: 0.6912, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 020, Train Loss: 0.6924, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 021, Train Loss: 0.6902, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 022, Train Loss: 0.6936, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 023, Train Loss: 0.6902, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 024, Train Loss: 0.6936, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 025, Train Loss: 0.6924, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 026, Train Loss: 0.6913, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 027, Train Loss: 0.6904, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 028, Train Loss: 0.6947, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 029, Train Loss: 0.6893, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 030, Train Loss: 0.6933, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 031, Train Loss: 0.6904, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 032, Train Loss: 0.6924, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 033, Train Loss: 0.6913, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 034, Train Loss: 0.6948, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 035, Train Loss: 0.6962, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 036, Train Loss: 0.6912, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 037, Train Loss: 0.6923, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 038, Train Loss: 0.6923, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 039, Train Loss: 0.6931, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 040, Train Loss: 0.6923, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 041, Train Loss: 0.6947, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 042, Train Loss: 0.6922, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 043, Train Loss: 0.6923, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 044, Train Loss: 0.6943, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 045, Train Loss: 0.6919, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 046, Train Loss: 0.6930, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 047, Train Loss: 0.6935, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 048, Train Loss: 0.6929, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 049, Train Loss: 0.6913, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 050, Train Loss: 0.6944, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 051, Train Loss: 0.6930, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 052, Train Loss: 0.6915, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 053, Train Loss: 0.6930, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 054, Train Loss: 0.6913, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 055, Train Loss: 0.6942, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 056, Train Loss: 0.6923, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 057, Train Loss: 0.6908, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 058, Train Loss: 0.6915, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 059, Train Loss: 0.6906, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 060, Train Loss: 0.6896, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 061, Train Loss: 0.6960, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 062, Train Loss: 0.6948, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 063, Train Loss: 0.6913, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 064, Train Loss: 0.6913, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 065, Train Loss: 0.6924, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 066, Train Loss: 0.6894, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 067, Train Loss: 0.6902, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 068, Train Loss: 0.6962, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 069, Train Loss: 0.6935, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 070, Train Loss: 0.6903, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 071, Train Loss: 0.6903, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 072, Train Loss: 0.6923, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 073, Train Loss: 0.6946, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 074, Train Loss: 0.6923, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 075, Train Loss: 0.6913, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 076, Train Loss: 0.6955, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 077, Train Loss: 0.6924, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 078, Train Loss: 0.6931, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 079, Train Loss: 0.6899, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 080, Train Loss: 0.6915, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 081, Train Loss: 0.6906, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 082, Train Loss: 0.6924, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 083, Train Loss: 0.6904, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 084, Train Loss: 0.6913, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 085, Train Loss: 0.6924, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 086, Train Loss: 0.6891, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 087, Train Loss: 0.6912, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 088, Train Loss: 0.6887, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 089, Train Loss: 0.6886, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 090, Train Loss: 0.6926, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 091, Train Loss: 0.6897, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 092, Train Loss: 0.6928, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 093, Train Loss: 0.6915, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 094, Train Loss: 0.6943, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 095, Train Loss: 0.6901, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 096, Train Loss: 0.6912, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 097, Train Loss: 0.6941, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 098, Train Loss: 0.6926, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 099, Train Loss: 0.6912, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "Epoch: 100, Train Loss: 0.6912, Train Acc: 0.5250, Test Acc: 0.4000\n",
            "\n",
            "--- Training complete ---\n",
            "Final Test Accuracy: 0.4000\n",
            "\n",
            "Category to Label Mapping:\n",
            "  category_A: 0\n",
            "  category_B: 1\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "id": "4ZDzRbN1JAk_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad5710a8-fa8a-4cb9-9856-8ad31d706988"
      }
    }
  ]
}